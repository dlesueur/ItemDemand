stores <- seq(1, 10)
items <- seq(1, 50)
store_items <- expand.grid(store = stores, item = items)
# Randomly sample 50 store-item combinations
random_store_items <- store_items[sample(nrow(store_items), 50), ]
# Filter train_data and test_data based on random store-item combinations
train_data_subset <- train_data %>%
semi_join(random_store_items, by = c("store", "item"))
test_data_subset <- test_data %>%
semi_join(random_store_items, by = c("store", "item"))
# nStores <- 2
# nItems <- 2
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "decimal")) %>%
step_mutate(date_dow = factor(date_dow),
date_month = factor(date_month),
sinDecimal = sin(date_decimal)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales))
# prophet_model <- prophet_reg() %>%
#   set_engine(engine = "prophet")
#
# cv_split <- time_series_split(train_data_subset, assess="3 months", cumulative=TRUE)
# cv_split %>%
#   tk_time_series_cv_plan() %>% #Put into a data frame
#   plot_time_series_cv_plan(date, sales, .interactive=FALSE)
#
# prophet_wf <- workflow() %>%
#   add_recipe(recipe) %>%
#   add_model(prophet_model) %>%
#   fit(data=training(cv_split))
#
# cv_results <- modeltime_calibrate(prophet_wf,
#                                   new_data=testing(cv_split))
#
# cv_results %>%
#   modeltime_forecast(new_data = testing(cv_split),
#                      actual_data = training(cv_split)) %>%
#   plot_modeltime_forecast(.interactive=FALSE)
#
# fullfit <- cv_results %>%
#   modeltime_refit(data = train_data_subset)
arima_model <- arima_reg(seasonal_period=12,
non_seasonal_ar=5,
non_seasonal_ma=5,
seasonal_ar=2,
seasonal_ma=2,
non_seasonal_differences=2,
seasonal_differences=2) %>%
set_engine("auto_arima")
cv_split <- time_series_split(train_data_subset, assess="3 months", cumulative=TRUE)
cv_split %>%
tk_time_series_cv_plan() %>% #Put into a data frame
plot_time_series_cv_plan(date, sales, .interactive=FALSE)
arima_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(arima_model) %>%
fit(data=training(cv_split))
cv_results <- modeltime_calibrate(arima_wf,
new_data=testing(cv_split))
cv_results %>%
modeltime_forecast(new_data = testing(cv_split),
actual_data = training(cv_split)) %>%
plot_modeltime_forecast(.interactive=FALSE)
fullfit <- cv_results %>%
modeltime_refit(data=storeItem1_train)
fullfit <- cv_results %>%
modeltime_refit(data=train_data_subset)
library(tidymodels)
library(embed)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
stores <- seq(1, 10)
items <- seq(1, 50)
store_items <- expand.grid(store = stores, item = items)
# Randomly sample 50 store-item combinations
random_store_items <- store_items[sample(nrow(store_items), 50), ]
# Filter train_data and test_data based on random store-item combinations
train_data_subset <- train_data %>%
semi_join(random_store_items, by = c("store", "item"))
test_data_subset <- test_data %>%
semi_join(random_store_items, by = c("store", "item"))
recipe <- recipe(sales ~ date, data = train_data_subset) %>%
step_date(date, features=c("dow", "month", "year", "decimal")) %>%
step_mutate(date_dow = factor(date_dow), date_month = factor(date_month),
sinDecimal = sin(date_decimal)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales))
tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(tree_model)
tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
folds <- vfold_cv(train_data_subset, v = 6, repeats=1)
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_params,
metrics=metric_set(smape))
library(tidymodels)
library(embed)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
stores <- seq(1, 10)
items <- seq(1, 50)
store_items <- expand.grid(store = stores, item = items)
# Randomly sample 50 store-item combinations
random_store_items <- store_items[sample(nrow(store_items), 50), ]
# Filter train_data and test_data based on random store-item combinations
train_data_subset <- train_data %>%
semi_join(random_store_items, by = c("store", "item"))
test_data_subset <- test_data %>%
semi_join(random_store_items, by = c("store", "item"))
recipe <- recipe(sales ~ date, data = train_data_subset) %>%
step_date(date, features=c("dow", "month", "year", "decimal")) %>%
step_mutate(date_dow = factor(date_dow), date_month = factor(date_month),
sinDecimal = sin(date_decimal)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales))
tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(tree_model)
tuning_params <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 5)
folds <- vfold_cv(train_data_subset, v = 6, repeats=1)
CV_results <- tree_wf %>%
tune_grid(resamples=folds,
grid=tuning_params,
metrics=metric_set(smape))
fullfit <- CV_results %>%
show_best(metric = "smape")
preds <- fullfit %>%
modeltime_forecast(new_data = test_data, actual_data = train_data_subset) %>%
filter(.key == "prediction") %>%
select(.value) %>%
mutate(id = test_data$id) %>%
rename(sales = .value) %>%
select(id, sales)
fullfit
preds <- fullfit %>%
modeltime_forecast(new_data = test_data, actual_data = train_data_subset) %>%
filter(.key == "prediction") %>%
select(.value) %>%
mutate(id = test_data$id) %>%
rename(sales = .value) %>%
select(id, sales)
fullfit <- CV_results %>%
show_best(metric = "smape") %>%
fit(data = train_data_subset)
best_params <- CV_results %>%
select_best(metric = "smape")
final_tree_model <- tree_wf %>%
finalize_workflow(best_params)
fitted_tree_model <- final_tree_model %>%
fit(data = train_data_subset)
calibrated_model <- modeltime_table(fitted_tree_model) %>%
modeltime_calibrate(new_data = train_data_subset)
preds <- calibrated_model %>%
modeltime_forecast(new_data = test_data_subset, actual_data = train_data_subset) %>%
filter(.key == "prediction") %>%
select(.value) %>%
mutate(id = test_data_subset$id) %>%
rename(sales = .value) %>%
select(id, sales)
# load in data and clean
data <- vroom("data.csv")
library(tidyverse)
library(tidymodels)
library(vroom)
library(skimr)
library(GGally)
library(ggplot2)
library(glmnet)
library(stacks)
library(recipes)
library(embed)
library(themis)
setwd("~/Documents/Stat348/Kobe")
# load in data and clean
data <- vroom("data.csv")
View(data)
train_data <- data[is.na(data$shot_made_flag) == FALSE, ]
test_data <- data[is.na(data$shot_made_flag) == TRUE, ]
View(test_data)
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
train_data <- read_csv("train.csv")
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
train_data <- read_csv("train.csv")
setwd("~/Documents/Stat348/ItemDemand")
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learning_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learn_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "doy")) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy))) %>%
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learn_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "doy")) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales)) %>%
step_rm(c(date, store, item)) %>%
step_normalize(all_numeric_predictors())
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- train_data %>%
filter(store==s, item==i)
storeItemTest <- test_data %>%
filter(store==s, item==i)
# cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative=TRUE)
# cv_split %>%
#   tk_time_series_cv_plan() %>% #Put into a data frame
#   plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(my_model) %>%
fit(data=storeItemTrain)
# cv_results <- modeltime_calibrate(prophet_wf,
#                                   new_data=testing(cv_split))
#
# cv_results %>%
#   modeltime_forecast(new_data = testing(cv_split),
#                      actual_data = training(cv_split)) %>%
#   plot_modeltime_forecast(.interactive=FALSE)
#
# fullfit <- cv_results %>%
#   modeltime_refit(data = storeItemTrain)
preds <- tree_wf %>%
predict(new_data = storeItemTest) %>%
mutate(id = storeItemTest$id) %>%
rename(sales = .pred) %>%
select(id, sales)
# Combine predictions
if(s == 1 & i == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
library(bonsai)
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
library(bonsai)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learn_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "doy")) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales)) %>%
step_rm(c(date, store, item)) %>%
step_normalize(all_numeric_predictors())
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- train_data %>%
filter(store==s, item==i)
storeItemTest <- test_data %>%
filter(store==s, item==i)
# cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative=TRUE)
# cv_split %>%
#   tk_time_series_cv_plan() %>% #Put into a data frame
#   plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(my_model) %>%
fit(data=storeItemTrain)
# cv_results <- modeltime_calibrate(prophet_wf,
#                                   new_data=testing(cv_split))
#
# cv_results %>%
#   modeltime_forecast(new_data = testing(cv_split),
#                      actual_data = training(cv_split)) %>%
#   plot_modeltime_forecast(.interactive=FALSE)
#
# fullfit <- cv_results %>%
#   modeltime_refit(data = storeItemTrain)
preds <- tree_wf %>%
predict(new_data = storeItemTest) %>%
mutate(id = storeItemTest$id) %>%
rename(sales = .pred) %>%
select(id, sales)
# Combine predictions
if(s == 1 & i == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
library(bonsai)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learn_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "doy")) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales)) %>%
step_rm(date) %>%
step_normalize(all_numeric_predictors())
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- train_data %>%
filter(store==s, item==i)
storeItemTest <- test_data %>%
filter(store==s, item==i)
# cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative=TRUE)
# cv_split %>%
#   tk_time_series_cv_plan() %>% #Put into a data frame
#   plot_time_series_cv_plan(date, sales, .interactive=FALSE)
prophet_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(my_model) %>%
fit(data=storeItemTrain)
# cv_results <- modeltime_calibrate(prophet_wf,
#                                   new_data=testing(cv_split))
#
# cv_results %>%
#   modeltime_forecast(new_data = testing(cv_split),
#                      actual_data = training(cv_split)) %>%
#   plot_modeltime_forecast(.interactive=FALSE)
#
# fullfit <- cv_results %>%
#   modeltime_refit(data = storeItemTrain)
preds <- tree_wf %>%
predict(new_data = storeItemTest) %>%
mutate(id = storeItemTest$id) %>%
rename(sales = .pred) %>%
select(id, sales)
# Combine predictions
if(s == 1 & i == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
library(tidymodels)
library(modeltime)
library(timetk)
library(readr)
library(embed)
library(patchwork)
library(vroom)
library(bonsai)
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
# nStores <- 2
# nItems <- 2
nStores <- max(train_data$store)
nItems <- max(train_data$item)
prophet_model <- prophet_reg() %>%
set_engine(engine = "prophet")
my_model <- boost_tree(tree_depth = 2,
trees = 1000,
learn_rate = .01) %>%
set_engine("lightgbm") %>%
set_mode("regression")
recipe <- recipe(sales ~ date, data = train_data) %>%
step_date(date, features=c("dow", "month", "year", "doy")) %>%
step_range(date_doy, min = 0, max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome=vars(sales)) %>%
step_rm(date) %>%
step_normalize(all_numeric_predictors())
for(s in 1:nStores){
for(i in 1:nItems){
storeItemTrain <- train_data %>%
filter(store==s, item==i)
storeItemTest <- test_data %>%
filter(store==s, item==i)
# cv_split <- time_series_split(storeItemTrain, assess="3 months", cumulative=TRUE)
# cv_split %>%
#   tk_time_series_cv_plan() %>% #Put into a data frame
#   plot_time_series_cv_plan(date, sales, .interactive=FALSE)
# prophet_wf <- workflow() %>%
#   add_recipe(recipe) %>%
#   add_model(my_model) %>%
#   fit(data=storeItemTrain)
tree_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(my_model) %>%
fit(data=storeItemTrain)
# cv_results <- modeltime_calibrate(prophet_wf,
#                                   new_data=testing(cv_split))
#
# cv_results %>%
#   modeltime_forecast(new_data = testing(cv_split),
#                      actual_data = training(cv_split)) %>%
#   plot_modeltime_forecast(.interactive=FALSE)
#
# fullfit <- cv_results %>%
#   modeltime_refit(data = storeItemTrain)
preds <- tree_wf %>%
predict(new_data = storeItemTest) %>%
mutate(id = storeItemTest$id) %>%
rename(sales = .pred) %>%
select(id, sales)
# Combine predictions
if(s == 1 & i == 1){
all_preds <- preds
} else {
all_preds <- bind_rows(all_preds, preds)
}
vroom_write(all_preds, "submisison.csv", delim = ",")
